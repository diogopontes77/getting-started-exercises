{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c4f5384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9de3bb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 21 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   checking_status         1000 non-null   object \n",
      " 1   duration                1000 non-null   int64  \n",
      " 2   credit_history          1000 non-null   object \n",
      " 3   purpose                 1000 non-null   object \n",
      " 4   credit_amount           1000 non-null   float64\n",
      " 5   savings_status          1000 non-null   object \n",
      " 6   employment              1000 non-null   object \n",
      " 7   installment_commitment  1000 non-null   int64  \n",
      " 8   personal_status         1000 non-null   object \n",
      " 9   other_parties           1000 non-null   object \n",
      " 10  residence_since         1000 non-null   int64  \n",
      " 11  property_magnitude      1000 non-null   object \n",
      " 12  age                     1000 non-null   int64  \n",
      " 13  other_payment_plans     1000 non-null   object \n",
      " 14  housing                 1000 non-null   object \n",
      " 15  existing_credits        1000 non-null   int64  \n",
      " 16  job                     1000 non-null   object \n",
      " 17  num_dependents          1000 non-null   int64  \n",
      " 18  own_telephone           1000 non-null   object \n",
      " 19  foreign_worker          1000 non-null   object \n",
      " 20  class                   1000 non-null   object \n",
      "dtypes: float64(1), int64(6), object(14)\n",
      "memory usage: 164.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('DatasetCredit-g.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "#print(df.head())\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c61e3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    checking_status  duration                  credit_history  \\\n",
      "0                <0         6  critical/other existing credit   \n",
      "1          0<=X<200        48                   existing paid   \n",
      "2       no checking        12  critical/other existing credit   \n",
      "3                <0        42                   existing paid   \n",
      "4                <0        24              delayed previously   \n",
      "..              ...       ...                             ...   \n",
      "995     no checking        12                   existing paid   \n",
      "996              <0        30                   existing paid   \n",
      "997     no checking        12                   existing paid   \n",
      "998              <0        45                   existing paid   \n",
      "999        0<=X<200        45  critical/other existing credit   \n",
      "\n",
      "                 purpose  credit_amount    savings_status  employment  \\\n",
      "0               radio/tv         1169.0  no known savings         >=7   \n",
      "1               radio/tv         5951.0              <100      1<=X<4   \n",
      "2              education         2096.0              <100      4<=X<7   \n",
      "3    furniture/equipment         7882.0              <100      4<=X<7   \n",
      "4                new car         4870.0              <100      1<=X<4   \n",
      "..                   ...            ...               ...         ...   \n",
      "995  furniture/equipment         1736.0              <100      4<=X<7   \n",
      "996             used car         3857.0              <100      1<=X<4   \n",
      "997             radio/tv          804.0              <100         >=7   \n",
      "998             radio/tv         1845.0              <100      1<=X<4   \n",
      "999             used car         4576.0        100<=X<500  unemployed   \n",
      "\n",
      "     installment_commitment     personal_status other_parties  ...  \\\n",
      "0                         4         male single          none  ...   \n",
      "1                         2  female div/dep/mar          none  ...   \n",
      "2                         2         male single          none  ...   \n",
      "3                         2         male single     guarantor  ...   \n",
      "4                         3         male single          none  ...   \n",
      "..                      ...                 ...           ...  ...   \n",
      "995                       3  female div/dep/mar          none  ...   \n",
      "996                       4        male div/sep          none  ...   \n",
      "997                       4         male single          none  ...   \n",
      "998                       4         male single          none  ...   \n",
      "999                       3         male single          none  ...   \n",
      "\n",
      "     property_magnitude age  other_payment_plans   housing existing_credits  \\\n",
      "0           real estate  67                 none       own                2   \n",
      "1           real estate  22                 none       own                1   \n",
      "2           real estate  49                 none       own                1   \n",
      "3        life insurance  45                 none  for free                1   \n",
      "4     no known property  53                 none  for free                2   \n",
      "..                  ...  ..                  ...       ...              ...   \n",
      "995         real estate  31                 none       own                1   \n",
      "996      life insurance  40                 none       own                1   \n",
      "997                 car  38                 none       own                1   \n",
      "998   no known property  23                 none  for free                1   \n",
      "999                 car  27                 none       own                1   \n",
      "\n",
      "                           job num_dependents  own_telephone class_binary  \\\n",
      "0                      skilled              1            yes            1   \n",
      "1                      skilled              1           none            0   \n",
      "2           unskilled resident              2           none            1   \n",
      "3                      skilled              2           none            1   \n",
      "4                      skilled              2           none            0   \n",
      "..                         ...            ...            ...          ...   \n",
      "995         unskilled resident              1           none            1   \n",
      "996  high qualif/self emp/mgmt              1            yes            1   \n",
      "997                    skilled              1           none            1   \n",
      "998                    skilled              1            yes            0   \n",
      "999                    skilled              1           none            1   \n",
      "\n",
      "     foreign  \n",
      "0          1  \n",
      "1          1  \n",
      "2          1  \n",
      "3          1  \n",
      "4          1  \n",
      "..       ...  \n",
      "995        1  \n",
      "996        1  \n",
      "997        1  \n",
      "998        1  \n",
      "999        1  \n",
      "\n",
      "[1000 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Binary labels \n",
    "mapping_foreign = {'no': 0, 'yes': 1}\n",
    "mapping = {'bad': 0, 'good': 1}\n",
    "\n",
    "\n",
    "df['class_binary'] = df['class'].map(mapping)\n",
    "df['foreign'] = df['foreign_worker'].map(mapping_foreign)\n",
    "df = df.drop(columns=['foreign_worker', 'class'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1034ee12",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "// df_binary = pd.get_dummies(data_frame[\"class\",\"foreign_worker\"], prefix='Binary')\n",
    "// print(df_bi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ae2648",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "// Isto preve o risco por agora, dps usar  amesma logica ma para credito, ja temos de criar outro modelo\n",
    "// Com o teste, verifica a previsao e verifica overfitting e isso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1b8ca17",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['checking_status', 'credit_history', 'purpose', 'savings_status',\n",
      "       'employment', 'personal_status', 'other_parties', 'property_magnitude',\n",
      "       'other_payment_plans', 'housing', 'job', 'own_telephone'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.select_dtypes(include=['object']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7dec9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, make_scorer\n",
    "\n",
    "\n",
    "profit_matrix = np.array([\n",
    "    [0,   -200],   \n",
    "    [-200, 100]    \n",
    "])\n",
    "\n",
    "def profit_score(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[1, 0])\n",
    "    return (cm * profit_matrix).sum() # soma final sendo o nosso objetivo ter o maximo possivel de lucro\n",
    "\n",
    "profit_scorer = make_scorer(profit_score, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af521fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.05      1.00      0.10         5\n",
      "           1       1.00      0.71      0.83       295\n",
      "\n",
      "    accuracy                           0.71       300\n",
      "   macro avg       0.53      0.85      0.47       300\n",
      "weighted avg       0.98      0.71      0.82       300\n",
      "\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time= 9.4min\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time= 9.4min\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time= 8.8min\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time= 8.8min\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time= 8.4min\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.1s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.1s\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time= 8.4min\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.1s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.1s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.1s\n",
      "[CV] END ....................C=0.1, gamma=scale, kernel=poly; total time=   0.1s\n",
      "[CV] END ....................C=0.1, gamma=scale, kernel=poly; total time=   0.1s\n",
      "[CV] END ....................C=0.1, gamma=scale, kernel=poly; total time=   0.1s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.1s\n",
      "[CV] END ....................C=0.1, gamma=scale, kernel=poly; total time=   0.1s\n",
      "[CV] END ....................C=0.1, gamma=scale, kernel=poly; total time=   0.1s\n",
      "[CV] END ....................C=0.1, gamma=scale, kernel=poly; total time=   0.1s\n",
      "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time= 9.6min\n",
      "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time= 9.6min\n",
      "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time= 8.8min\n",
      "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time= 8.8min\n",
      "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time= 8.3min\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.1s\n",
      "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time= 8.3min\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.1s\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.1s\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.1s\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.1s\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.1s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_x = df.drop(columns=[\"class_binary\"])\n",
    "X= data_x\n",
    "y= df[\"class_binary\"]\n",
    "\n",
    "categorical_features = X.select_dtypes(include=[\"object\"]).columns\n",
    "numeric_features = X.select_dtypes(exclude=[\"object\"]).columns\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "        (\"num\", \"passthrough\", numeric_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "rf_classifier.fit(X_train_processed, y_train)\n",
    "\n",
    "y_pred = rf_classifier.predict(X_test_processed)\n",
    "\n",
    "print(classification_report(y_pred, y_test))\n",
    "\n",
    "# Nos nao priorizamos a acuracia, mas sim o lucro\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid_search = {\n",
    "    'n_estimators': [100, 200, 150],\n",
    "    'max_depth': [None, 5, 10, 20, 50],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Use the profit_scorer defined earlier\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_classifier,\n",
    "    param_grid=param_grid_search,\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    scoring=profit_scorer,\n",
    "    error_score='raise'\n",
    ")\n",
    "grid_search.fit(X_train_processed, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Estimator:\", grid_search.best_estimator_)\n",
    "print(\"Best Profit Score:\", grid_search.best_score_)\n",
    "#Má previsao e recall para valores 0, ver o que posso fazer para melhorar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45e2bb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.74      0.47        42\n",
      "           1       0.95      0.77      0.85       258\n",
      "\n",
      "    accuracy                           0.76       300\n",
      "   macro avg       0.64      0.75      0.66       300\n",
      "weighted avg       0.86      0.76      0.79       300\n",
      "\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=150; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=150; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=150; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=150; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=150; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=150; total time=   0.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=50, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=50, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=50, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=50, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=50, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=50, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=50, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=5, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=50, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=50, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=50, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=50, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=50, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "Best Parameters: {'n_estimators': 150, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': None}\n",
      "Best Estimator: RandomForestClassifier(min_samples_split=5, n_estimators=150, random_state=42)\n",
      "Best Profit Score: -8466.666666666666\n",
      "Best Parameters: {'n_estimators': 150, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': None}\n",
      "Best Estimator: RandomForestClassifier(min_samples_split=5, n_estimators=150, random_state=42)\n",
      "Best Profit Score: -8466.666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "data_x = df.drop(columns=[\"class_binary\"])\n",
    "X= data_x\n",
    "y= df[\"class_binary\"]\n",
    "\n",
    "categorical_features = X.select_dtypes(include=[\"object\"]).columns\n",
    "numeric_features = X.select_dtypes(exclude=[\"object\"]).columns\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "        (\"num\", \"passthrough\", numeric_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "rf_classifier.fit(X_train_processed, y_train)\n",
    "\n",
    "y_pred = rf_classifier.predict(X_test_processed)\n",
    "\n",
    "print(classification_report(y_pred, y_test))\n",
    "\n",
    "# Nos nao priorizamos a acuracia, mas sim o lucro\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 150],\n",
    "    'max_depth': [None, 5, 10, 20, 50],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Use the profit_scorer defined earlier\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_classifier,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    scoring=profit_scorer,\n",
    "    error_score='raise',\n",
    "    random_state=42\n",
    ")\n",
    "random_search.fit(X_train_processed, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Estimator:\", random_search.best_estimator_)\n",
    "print(\"Best Profit Score:\", random_search.best_score_)\n",
    "#Má previsao e recall para valores 0, ver o que posso fazer para melhorar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe05b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(X_test)\n",
    "shap.initjs()\n",
    "\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870be5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(RandomForestClassifier(),\n",
    "                                   param_grid= param_grid_search)\n",
    "random_search.fit(X_train, y_train)\n",
    "print(random_search.best_estimator_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18956dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def profit_function(y_true, y_pred):\n",
    "    within_margin = np.abs(y_true - y_pred) / y_true < 0.3\n",
    "    return 100 * within_margin.sum()   # total profit\n",
    "\n",
    "profit_scorer = make_scorer(profit_function, greater_is_better=True)\n",
    "\n",
    "#Isto é para o outro modelo que em vez de avaliar o risco. avalia o credito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680d2a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aqui o P é o valor previsto do nosso modelo, o Y é o suposto \"desejado\", não sei bem o que quer dizer\n",
    "#tenho que fazer dois modelos, um para prever o risco e outro para prever o credit amount requested mesmo para utilizar na função como o P\n",
    "\n",
    "def calculate_f(Y, P):\n",
    "    if abs(Y - P) / Y < 0.3:\n",
    "        return 100\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a768067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function with different Y and P values\n",
    "Y = 100\n",
    "P = 70\n",
    "\n",
    "result = calculate_f(Y, P)\n",
    "print(f\"f({Y}, {P}) = {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1973200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizar XGBoost regression como usaste em DAA, ver como fizeste isso em DAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb19dbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# falta me só ver como é que vou passar as labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
